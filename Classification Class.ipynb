{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T18:33:00.413819Z",
     "start_time": "2020-01-21T18:33:00.318200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scikitplot as skplt\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, StratifiedKFold, GridSearchCV, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:23:16.447633Z",
     "start_time": "2020-01-21T17:23:16.398971Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class full_classification:\n",
    "    \"\"\"A class which automatically does all classification models and gridsearches for you (logisitic default). Remember to input baseline figure and decide if you want standardisation.\n",
    "    Note: when you run a new model it will overwrite the previous model. You can access the current model with .model and .model_des.\n",
    "    Created by LukeBetham\"\"\"\n",
    "\n",
    "    def __init__(self, X, y, baseline=0, standardize=\"none\", test_size=0.15, folds=6, shuffle=True):\n",
    "\n",
    "        # Set up the KFolds\n",
    "        self.folds = folds\n",
    "        self.shuffle = shuffle\n",
    "        # Option for bolding print text\n",
    "        self.BOLD = '\\033[1m'\n",
    "        self.END = '\\033[0m'\n",
    "        # Create train-test if selected\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.baseline = baseline\n",
    "        self.test = test_size\n",
    "        if self.test != 0:\n",
    "            self.X, self.X_test, self.y, self.y_test = train_test_split(\n",
    "                self.X, self.y, test_size=self.test, random_state=66)\n",
    "        # Standardise the data if selected\n",
    "        if standardize != 'none':\n",
    "            scaler = StandardScaler()\n",
    "            self.X = pd.DataFrame(\n",
    "                scaler.fit_transform(self.X), columns=X.columns)\n",
    "            if self.test != 0:\n",
    "                self.X_test = pd.DataFrame(\n",
    "                    scaler.transform(self.X_test), columns=X.columns)\n",
    "        # Call the 3 standard models\n",
    "        self.knn_model(5)\n",
    "        self.decision_tree_model()\n",
    "        self.logistic_model()\n",
    "\n",
    "    def logistic_model(self, Logistic=LogisticRegression(penalty='none', max_iter=1000)):\n",
    "        # Set up Logistic Regresssion\n",
    "        self.model = Logistic\n",
    "        self.model_des = \"Logistic Regression Model\"\n",
    "        self.model_calc()\n",
    "        print(\"Run .coefs() to see coef dataframe\\nTime Elapsed = \", round(self.elaspsed, 2),\n",
    "              'secs - grid will take ~', round(self.elaspsed*30, 2), 'minutes to run.\\n')\n",
    "\n",
    "    def knn_model(self, k='all', weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None):\n",
    "        if k != 'all':\n",
    "            # set up KNN model\n",
    "            self.model = KNeighborsClassifier(n_neighbors=k, weights=weights, algorithm=algorithm,\n",
    "                                              leaf_size=leaf_size, p=p, metric=metric, metric_params=metric_params, n_jobs=n_jobs)\n",
    "            self.model_des = \"K Neighbors Model\"\n",
    "            self.model_calc()\n",
    "            print(\"Set k='all' to run full set of ks and graph.\\nTime Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~', round(\n",
    "                self.elaspsed*7, 2), 'minutes to run - and all ks', round(int(len(self.y)*(1-(1/self.folds))-1)*self.elaspsed/120, 2), 'mins\\n')\n",
    "\n",
    "        else:\n",
    "            # run KNN for all possible Ks and graph them\n",
    "            self.scores = []\n",
    "            self.max_k = int(len(self.y)*(1-(1/self.folds))-1)\n",
    "            for k in range(1, self.max_k):\n",
    "                knn = KNeighborsClassifier(n_neighbors=k, weights=weights, algorithm=algorithm,\n",
    "                                           leaf_size=leaf_size, p=p, metric=metric, metric_params=metric_params, n_jobs=n_jobs)\n",
    "                self.scores.append(np.mean(cross_val_score(knn, self.X, self.y, cv=KFold(\n",
    "                    self.folds, shuffle=self.shuffle, random_state=66))))\n",
    "            self.knn_best = self.scores.index(np.max(self.scores))+1\n",
    "            plt.plot(range(1, self.max_k), self.scores, label='Mean CV Scores')\n",
    "            plt.hlines(self.baseline, 1, self.max_k, label='baseline')\n",
    "            plt.xlabel('k')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.legend(loc=[1.1, 0])\n",
    "            print(self.BOLD + \"Highest KNN Score:\" + self.END, self.knn_best)\n",
    "            plt.show()\n",
    "\n",
    "    def decision_tree_model(self, print_tree=\"y\", DecisionTree=DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, random_state=66)):\n",
    "        # set up decision tree model\n",
    "        self.model = DecisionTree\n",
    "        self.model_des = \"Decision Tree Model\"\n",
    "        self.model_calc()\n",
    "        print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "              round(self.elaspsed*50, 2), 'minutes to run.\\n')\n",
    "        if print_tree == 'y':\n",
    "            # insert code to view tree here\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def random_forest_model(self, forest=RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, random_state=66)):\n",
    "        self.model = forest\n",
    "        self.model_des = \"Random Forest Model\"\n",
    "        self.model_calc()\n",
    "        print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "              round(self.elaspsed*9.5, 2), 'minutes to run.\\n')\n",
    "\n",
    "    def boosting_model(self, plot_it=True, estimators=100, base_estimator=DecisionTreeClassifier(max_depth=3,random_state=66)):\n",
    "        self.model = AdaBoostClassifier(\n",
    "            base_estimator=base_estimator, n_estimators=estimators, algorithm='SAMME', random_state=66)\n",
    "        self.model_des = \"Boosting Model\"\n",
    "        self.model_calc()\n",
    "        print(\"Time Elapsed = \", round(self.elaspsed, 2), 'secs - grid will take ~',\n",
    "              round(self.elaspsed*2.5, 2), 'minutes to run.\\n')\n",
    "        # plot\n",
    "        if plot_it == True:\n",
    "            plt.plot(list(self.model.staged_score(self.X, self.y)),\n",
    "                     label='training score', lw=2)\n",
    "            plt.plot(list(self.model.staged_score(\n",
    "                self.X_test, self.y_test)), label='test score', lw=2)\n",
    "            plt.xlabel('iteration')\n",
    "            plt.ylabel('score')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def coefs(self):\n",
    "        self.dfc = pd.DataFrame(self.coef, columns=self.X.columns)\n",
    "        return self.dfc\n",
    "\n",
    "    def model_calc(self):\n",
    "        # fit model\n",
    "        t0 = time.time()\n",
    "        self.model.fit(self.X, self.y)\n",
    "        self.sc = self.model.score(self.X, self.y)\n",
    "        self.cvs = cross_val_score(self.model, self.X, self.y, cv=KFold(\n",
    "            self.folds, shuffle=self.shuffle, random_state=66)).mean()\n",
    "        # Get test score\n",
    "        if self.test != 0:\n",
    "            self.sct = self.model.score(self.X_test, self.y_test)\n",
    "            self.sctp = str(round(self.sct, 4))+\" - better than baseline by \" + \\\n",
    "                str(round(self.sct-self.baseline, 4))\n",
    "        else:\n",
    "            self.sctp = None\n",
    "        t1 = time.time()\n",
    "        self.elaspsed = t1-t0\n",
    "        # show the results from the classification model\n",
    "        print(self.BOLD + self.model_des, 'Test\\nModel Score:' + self.END, round(self.sc, 4), \"- better than baseline by\", round(self.sc-self.baseline, 4),\n",
    "              self.BOLD + '\\nCV Fold Score:' +\n",
    "              self.END, round(\n",
    "                  self.cvs, 4), \"- better than baseline by\", round(self.cvs-self.baseline, 4),\n",
    "              self.BOLD + \"\\nModel Test Score:\" + self.END, self.sctp)\n",
    "        try:\n",
    "            self.coef = self.model.coef_\n",
    "            self.coefs\n",
    "        except:\n",
    "            pass\n",
    "        print(\"Use .gridsearch() to run full regularisation tests using all default for current model.\",\n",
    "              \"\\nUse .knn_model() or .logistic_model() or .decision_tree_model() to change model and specify paramters.\")\n",
    "\n",
    "    def gridsearch(self, params='default'):\n",
    "        \"\"\"A function which automatically runs a gridsearch on your selected model. Returns model_grid model with best parameters.\n",
    "        Defaults for Logistic (600 iterations): {'penalty': ['l1', 'l2', 'elasticnet'], 'solver': ['saga'], 'C': np.logspace(-5, 5, 5), 'l1_ratio': np.linspace(0.0001, 1, 4)}\n",
    "        Defaults for KNN: self.params (100 iterations) = {'n_neighbors':range(1,20,1), 'weights':['uniform','distance'], 'p':[1,2]}         \n",
    "        Defaults for Decision Tree (1000 iterations) = {'criterion':['gini','entropy'],'max_depth': [None,5,6,7,8],'max_features':['auto'],'splitter':['best','random'],'min_samples_split':[2,3,4,5],'ccp_alpha':[0.0,0.0001,0.001,.01,.1,1,10,100],'class_weight':[None,'balanced']}        \n",
    "        Defaults for Random Forest (575 iterations) = {'n_estimators':[100,200,500], 'criterion':['gini':'entropy'], 'max_depth':[None], 'min_samples_split':[2,4,6],\"max_features\":[\"auto\",\"log2\"],'oob_score':[True,False],'warm_start':[True,False],'ccp_alpha'=[0.0,0.5,1,10]}      \n",
    "        Defaults for Boosting Model  (150 iterations)  = {\"learning_rate\": [0.05, 0.25, 0.5, 0.75, 1], \"max_depth\":[1,2,3,4,5],\"max_features\":[\"auto\",\"log2\"],\"n_estimators\":[100,200,500]}      \n",
    "                \"\"\"\n",
    "        # setting the default parameters if not set by user\n",
    "        if params == 'default':\n",
    "            if self.model_des == \"Logistic Regression Model\":\n",
    "                self.params = {'penalty': ['l1', 'l2', 'elasticnet'], 'solver': ['saga'], 'C': np.logspace(-5, 5, 5), 'l1_ratio': np.linspace(0.0001, 1, 4)}\n",
    "            elif self.model_des == \"K Neighbors Model\":\n",
    "                self.params = {'n_neighbors': range(1, 20, 1), 'weights': ['uniform', 'distance'], 'p': [1, 2]}\n",
    "            elif self.model_des == \"Decision Tree Model\":\n",
    "                self.params = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 5, 6, 7, 8], 'max_features': ['auto'], 'splitter': [\n",
    "                    'best', 'random'], 'min_samples_split': [2, 3, 4, 5], 'ccp_alpha': [0.0, 0.0001, 0.001, .01, .1, 1, 10, 100], 'class_weight': [None, 'balanced']}\n",
    "            elif self.model_des == \"Random Forest Model\":\n",
    "                self.params = {'n_estimators':[100,200,500], 'criterion':['gini','entropy'], 'max_depth':[None], 'min_samples_split':[2,6],\"max_features\":[\"auto\",\"log2\"],\n",
    "                               'oob_score':[True,False],'warm_start':[True,False],'ccp_alpha':[0.0,0.5,1]}\n",
    "            elif self.model_des == \"Boosting Model\": \n",
    "                self.params = {\"learning_rate\": [0.05, 0.25, 0.5, 0.75, 1], 'base_estimator':[DecisionTreeClassifier(max_depth=1),DecisionTreeClassifier(max_depth=2),DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=4),DecisionTreeClassifier(max_depth=5)],\n",
    "                               'algorithm':['SAMME'],\"n_estimators\":[100,200,500,1000]}\n",
    "        else:\n",
    "            self.params = params\n",
    "\n",
    "        # setup the gridsearch\n",
    "        self.grid = GridSearchCV(self.model, self.params, verbose=1, cv=KFold(\n",
    "            self.folds, shuffle=self.shuffle, random_state=66))\n",
    "        self.grid.fit(self.X, self.y)\n",
    "        self.gsc = self.grid.best_score_\n",
    "        self.best = self.grid.best_params_\n",
    "        self.model = self.grid.best_estimator_\n",
    "        self.model_des = self.model_des + \" Grid Search:\"\n",
    "        try:\n",
    "            self.coef = self.grid.best_estimator_.coef_\n",
    "        except:\n",
    "            pass\n",
    "        # Check test score for grid\n",
    "        try:\n",
    "            self.sct = self.grid.best_estimator_.score(\n",
    "                self.X_test, self.y_test)\n",
    "            self.sctp = str(round(self.sct, 4))+\" - better than baseline by \" + \\\n",
    "                str(round(self.sct-self.baseline, 4))\n",
    "        except:\n",
    "            self.sctp = None\n",
    "        # Print Grid results\n",
    "        print(self.BOLD + self.model_des + self.END)\n",
    "        print(self.BOLD + \"Best Mean CV Model Score:\" + self.END, round(self.gsc, 4), \"- which is better than baseline by\",\n",
    "              round(self.gsc-self.baseline, 4), self.BOLD + \"\\nModel Test Score:\" + self.END, self.sctp)\n",
    "        print(self.BOLD + 'Grid Best Parameters:\\n' + self.END, self.best)\n",
    "        print(self.BOLD + '\\nSearch Parameters:\\n' + self.END, self.params)\n",
    "        self.coefs()\n",
    "\n",
    "    def matrix_n_graphs(self):\n",
    "        print(self.BOLD + self.model_des, \"on X_test\" + self.END)\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        skplt.metrics.plot_confusion_matrix(\n",
    "            self.y_test, self.y_pred, figsize=(8, 8), labels=[0, 1, 2, 3], normalize=True)\n",
    "        plt.ylim([-0.5, len(self.y.unique())-0.5])\n",
    "        plt.show()\n",
    "        cmap = ListedColormap(sns.color_palette(\"husl\", 3))\n",
    "        skplt.metrics.plot_roc(self.y_test, self.model.predict_proba(self.X_test), plot_micro=False,\n",
    "                               plot_macro=False, title_fontsize=20, text_fontsize=16, figsize=(8, 8), cmap=cmap)\n",
    "        plt.show()\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        skplt.metrics.plot_precision_recall(self.y_test, self.model.predict_proba(\n",
    "            self.X_test), plot_micro=False, title_fontsize=20, text_fontsize=16, cmap=cmap, ax=ax)\n",
    "        ax.legend(loc=[1.1, 0])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
